
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a final year PhD student at the Department of Statistics in Oxford, supervised by Arnaud Doucet and George Deligiannidis.\nMy current research lies in statistical learning theory and its applications to modern deep learning algorithms. More precisely, I have been working mostly on generalisation bounds in the PAC-Bayesian and information-theoretic frameworks, and on the Gaussian limit for infinitely wide and deep neural networks. I am particularly interested in topics at the intersection between probability, information theory, and large deviations theory, such as transport-entropy inequalities and concentration inequalities, and their connections with the generalisation problem in learning theory.\nPrior to arriving in Oxford, I obtained a Bachelor’s degree in Physics at the University of Pavia (Italy) and a Master’s degree in theoretical Physics at the École Normale Supérieure in Paris, where I was a member of the International Selection. I have also followed several pure Mathematics classes from the University Pierre and Marie Curie (now Sorbonne University) in Paris.\nThis year I had the chance to visit and collaborate with Benjamin Guedj and his team at the UCL Centre for Artificial Intelligence in London.\n Find here my CV (updated: September, 2022).--- ","date":1661990400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1661990400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a final year PhD student at the Department of Statistics in Oxford, supervised by Arnaud Doucet and George Deligiannidis.\nMy current research lies in statistical learning theory and its applications to modern deep learning algorithms.","tags":null,"title":"Eugenio Clerico","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a66d616325fc1c22a2dbe898f5ae9d09","permalink":"https://example.com/old_home/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/skills/","section":"old_home","summary":"","tags":null,"title":"Skills","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16dbee8f5696244fe3855fe8808d5762","permalink":"https://example.com/old_home/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/experience/","section":"old_home","summary":"","tags":null,"title":"Experience","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"109f7b327168675adb8b15da4b9e4f84","permalink":"https://example.com/old_home/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/accomplishments/","section":"old_home","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24207ec0df3bcbeca1910173bce97a2f","permalink":"https://example.com/old_home/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/posts/","section":"old_home","summary":"","tags":null,"title":"Recent Posts","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a3a73a5eac92782709f86202418bc22","permalink":"https://example.com/old_home/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/projects/","section":"old_home","summary":"","tags":null,"title":"Projects","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5f57af6145ff7c4af04ff60ec4b69318","permalink":"https://example.com/old_home/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/featured/","section":"old_home","summary":"","tags":null,"title":"Featured Publications","type":"old_home"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8a84f62e19347d2ec03218bc1f7a3093","permalink":"https://example.com/old_home/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/old_home/tags/","section":"old_home","summary":"","tags":null,"title":"Popular Topics","type":"old_home"},{"authors":["Eugenio Clerico","George Deligiannidis","Benjamin Guedj","Arnaud Doucet"],"categories":[],"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"ac59cf0a34c90dfa20e7bdc310721c5e","permalink":"https://example.com/publication/pacbayesgd/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/pacbayesgd/","section":"publication","summary":"We establish a disintegrated PAC-Bayesian bound, for classifiers that are trained via continuous-time (non-stochastic) gradient descent. Contrarily to what is standard in the PAC-Bayesian setting, our result applies to a training algorithm that is deterministic, conditioned on a random initialisation, without requiring any *de-randomisation* step. We provide a broad discussion of the main features of the bound that we propose, and we study analytically and empirically its behaviour on linear models, finding promising results.","tags":[],"title":"A PAC-Bayesian bound for deterministic algorithms","type":"publication"},{"authors":[],"categories":null,"content":"","date":1656720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656720000,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/colt-2022/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/colt-2022/","section":"event","summary":"Oral ([Chained generalisation bounds](publication/chained))","tags":[],"title":"COLT 2022","type":"event"},{"authors":["Eugenio Clerico","Amitis Shidani","George Deligiannidis","Arnaud Doucet"],"categories":[],"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"3a674505a1cd127d1fd5d51eb27bbbcb","permalink":"https://example.com/publication/chained/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/publication/chained/","section":"publication","summary":"This work discusses how to derive upper bounds for the expected generalisation error of supervised learning algorithms by means of the chaining technique. By developing a general theoretical framework, we establish a duality between generalisation bounds based on the regularity of the loss function, and their chained counterparts, which can be obtained by lifting the regularity assumption from the loss onto its gradient. This allows us to re-derive the chaining mutual information bound from the literature, and to obtain novel chained information-theoretic generalisation bounds, based on the Wasserstein distance and other probability metrics. We show on some toy examples that the chained generalisation bound can be significantly tighter than its standard counterpart, particularly when the distribution of the hypotheses selected by the algorithm is very concentrated.","tags":[],"title":"Chained generalisation bounds","type":"publication"},{"authors":[],"categories":null,"content":"","date":1648480619,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648480619,"objectID":"240721f4fcd272b3bda0fde2fb183e55","permalink":"https://example.com/talk/aistats-2022/","publishdate":"2022-03-28T16:16:59+01:00","relpermalink":"/talk/aistats-2022/","section":"event","summary":"Poster ([Conditionally Gaussian PAC-Bayes](publication/condgauss))","tags":[],"title":"AISTATS 2022","type":"event"},{"authors":["Eugenio Clerico","George Deligiannidis","Arnaud Doucet"],"categories":[],"content":"","date":1643737544,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643737544,"objectID":"5e004ac8152320f1fdb2908a44516d43","permalink":"https://example.com/publication/condgauss/","publishdate":"2022-02-01T18:45:44+01:00","relpermalink":"/publication/condgauss/","section":"publication","summary":"Recent studies have empirically investigated different methods to train stochastic neural networks on a classification task by optimising a PAC-Bayesian bound via stochastic gradient descent. Most of these procedures need to replace the misclassification error with a surrogate loss, leading to a mismatch between the optimisation objective and the actual generalisation bound. The present paper proposes a novel training algorithm that optimises the PAC-Bayesian bound, without relying on any surrogate loss. Empirical results show that this approach outperforms currently available PAC-Bayesian training methods.","tags":[],"title":"Conditionally Gaussian PAC-Bayes","type":"publication"},{"authors":["Eugenio Clerico","George Deligiannidis","Arnaud Doucet"],"categories":[],"content":"","date":1622569957,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622569957,"objectID":"9fa7c8f7e9f3c01e1716b6c608903790","permalink":"https://example.com/publication/wide/","publishdate":"2021-06-01T18:52:37+01:00","relpermalink":"/publication/wide/","section":"publication","summary":"The limit of infinite width allows for substantial simplifications in the analytical study of overparameterized neural networks. With a suitable random initialization, an extremely large network is well approximated by a Gaussian process, both before and during training. In the present work, we establish a similar result for a simple stochastic architecture whose parameters are random variables. The explicit evaluation of the output distribution allows for a PAC-Bayesian training procedure that directly optimizes the generalization bound. For a large but finite-width network, we show empirically on MNIST that this training approach can outperform standard PAC-Bayesian methods.","tags":[],"title":"Wide stochastic networks: Gaussian limit and PAC-Bayesian training","type":"publication"},{"authors":[],"categories":null,"content":"","date":1618327019,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618327019,"objectID":"b439c93c5c5854580d7ed48b90a0bf28","permalink":"https://example.com/talk/aistats-2021/","publishdate":"2022-03-28T16:16:59+01:00","relpermalink":"/talk/aistats-2021/","section":"event","summary":"Oral and poster ([Stable ResNet](publication/stable))","tags":[],"title":"AISTATS 2021","type":"event"},{"authors":["Soufiane Hayou","Eugenio Clerico","Bobby He","George Deligiannidis","Arnaud Doucet","Judith Rousseau"],"categories":[],"content":"","date":1614621424,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614621424,"objectID":"5ecd4dd65d19840c2e773d7ebdb757e5","permalink":"https://example.com/publication/stable/","publishdate":"2021-03-01T18:57:04+01:00","relpermalink":"/publication/stable/","section":"publication","summary":"Deep ResNet architectures have achieved state of the art performance on many tasks. While they solve the problem of gradient vanishing, they might suffer from gradient exploding as the depth becomes large (Yang et al. 2017). Moreover, recent results have shown that ResNet might lose expressivity as the depth goes to infinity (Yang et al. 2017, Hayou et al. 2019). To resolve these issues, we introduce a new class of ResNet architectures, calledStable ResNet, that have the property of stabilizing the gradient while ensuring expressivity in the infinite depth limit.","tags":[],"title":"Stable ResNet","type":"publication"}]